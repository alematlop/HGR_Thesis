{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgSZvsJ3c6OZ"
      },
      "outputs": [],
      "source": [
        "## module imports ##\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import math\n",
        "import copy\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn import svm, metrics, preprocessing\n",
        "from functools import partial\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "import statistics\n",
        "import random\n",
        "import seaborn as sn\n",
        "from scikeras.wrappers import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preproc_signals(moves, win_sz, overlap):  ## Main processing function - load raw data and organise steps ##\n",
        "    \n",
        "    feat_mat = np.array([])\n",
        "    \n",
        "    for subject in range(1,23):\n",
        "        \n",
        "        ## retrieve signals\n",
        "        \n",
        "        ex1_mat_file = 'S' + str(subject) + '_E1_A1.mat'\n",
        "        ex1_mat = scipy.io.loadmat(ex1_mat_file)\n",
        "        \n",
        "        ex2_mat_file = 'S' + str(subject) + '_E2_A1.mat'\n",
        "        ex2_mat = scipy.io.loadmat(ex2_mat_file)\n",
        "        \n",
        "\n",
        "        emg = np.concatenate((np.array(ex1_mat['emg']), np.array(ex2_mat['emg'])), axis=0)\n",
        "        \n",
        "        acc = np.concatenate((np.array(ex1_mat['acc']), np.array(ex2_mat['acc'])), axis=0)\n",
        "        \n",
        "        gyro = np.concatenate((np.array(ex1_mat['gyro']), np.array(ex2_mat['gyro'])), axis=0)\n",
        "        \n",
        "        mag = np.concatenate((np.array(ex1_mat['mag']), np.array(ex2_mat['mag'])), axis=0)\n",
        "        \n",
        "        stim = np.concatenate((np.array(ex1_mat['restimulus']), np.array(ex2_mat['restimulus'])), axis=0)\n",
        "        \n",
        "        print('S' + str(subject) + ' concatenation complete')\n",
        "        \n",
        "        ## find samples where movement is present\n",
        "\n",
        "        stim[np.where(stim!=0)] = 1\n",
        "        \n",
        "        ## remove dc components of signals\n",
        "        \n",
        "        remove_DC_component(emg, stim)\n",
        "        \n",
        "        remove_DC_component(acc, stim)\n",
        "        \n",
        "        remove_DC_component(gyro, stim)\n",
        "        \n",
        "        remove_DC_component(mag, stim)\n",
        "        \n",
        "        print('S' + str(subject) + ' dc removal complete')\n",
        "        \n",
        "        ## filter signals using a butterworth filter\n",
        "        \n",
        "        freq_filt(emg, 'bp')\n",
        "        \n",
        "        freq_filt(acc, 'lp')\n",
        "        \n",
        "        freq_filt(gyro, 'lp')\n",
        "        \n",
        "        freq_filt(mag, 'lp')\n",
        "        \n",
        "        print('S' + str(subject) + ' filtering complete')\n",
        "        \n",
        "        ## retain samples of signals where movement is present\n",
        "        \n",
        "        emg *= stim\n",
        "        \n",
        "        acc *= stim\n",
        "        \n",
        "        gyro *= stim\n",
        "        \n",
        "        mag *= stim\n",
        "        \n",
        "        print('S' + str(subject) + ' cleaning complete')\n",
        "        \n",
        "        ## split stream into individual movements\n",
        "        \n",
        "        feat_mat = split(emg, acc, gyro, mag, stim, subject, moves, feat_mat, win_sz, overlap)\n",
        "        \n",
        "        print('\\nS' + str(subject) + ' splitting and feature extraction complete\\n')\n",
        "    \n",
        "    print('\\nPreprocessing task complete')\n",
        "        \n",
        "    return feat_mat"
      ],
      "metadata": {
        "id": "tGw5a_03dDZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(emg, acc, gyro, mag, stim, subnum, moves, feat_mat, win_sz, overlap):  ## Split raw data stream into individual movements and repetitions ##\n",
        "    \n",
        "    i = -1\n",
        "    \n",
        "    m = 1\n",
        "    r = 1\n",
        "    \n",
        "    while i < len(stim)-1:\n",
        "        \n",
        "        i += 1\n",
        "        \n",
        "        if stim[i] != 0:\n",
        "            \n",
        "            temp = i\n",
        "            \n",
        "            while stim[i] != 0:\n",
        "                \n",
        "                i += 1\n",
        "                \n",
        "                if i >= len(stim): break\n",
        "            \n",
        "            if m in moves:\n",
        "                \n",
        "                mov_rep_mat = np.concatenate((\n",
        "                    create_vector_matrix_with_overlap(emg[temp:i], m, r, subnum, win_sz, overlap, 'emg'),\n",
        "                    create_vector_matrix_with_overlap(acc[temp:i], m, r, subnum, win_sz, overlap, 'acc'),\n",
        "                    create_vector_matrix_with_overlap(gyro[temp:i], m, r, subnum, win_sz, overlap, 'gyro'),\n",
        "                    create_vector_matrix_with_overlap(mag[temp:i], m, r, subnum, win_sz, overlap, 'mag')\n",
        "                    ),\n",
        "                    axis = 1\n",
        "                    )\n",
        "                \n",
        "                \n",
        "                if feat_mat.size == 0:\n",
        "                    \n",
        "                    feat_mat = mov_rep_mat\n",
        "                    \n",
        "                else:\n",
        "                    \n",
        "                    feat_mat = np.append(feat_mat, mov_rep_mat, axis=0)\n",
        "                    \n",
        "            \n",
        "                print('['+str(m)+'-'+str(r)+']', end = '')\n",
        "            \n",
        "            \n",
        "            r += 1\n",
        "            \n",
        "            if r == 7:\n",
        "                \n",
        "                m += 1\n",
        "                \n",
        "                r = 1\n",
        "                   \n",
        "    return feat_mat"
      ],
      "metadata": {
        "id": "2TCSsOECePEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_matrix_with_overlap(signal, m, r, subnum, win_sz, overlap, tp):  ## Build feature matrix for given movement and repetition, connect fragment to main feature matrix ##\n",
        "    \n",
        "    mat = []\n",
        "    \n",
        "    if tp == 'emg':\n",
        "        \n",
        "        features = [mean_absolute_value,\n",
        "                    zero_crossings,\n",
        "                    slope_sign_changes,\n",
        "                    waveform_length,\n",
        "                    root_mean_square,\n",
        "                    variance]\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        features = [mean_absolute_value,\n",
        "                    zero_crossings,\n",
        "                    slope_sign_changes,\n",
        "                    waveform_length,\n",
        "                    root_mean_square,\n",
        "                    variance]\n",
        "    \n",
        "    for w in range(0, len(signal), int((1-overlap)*win_sz)):\n",
        "        \n",
        "        vec = []\n",
        "        \n",
        "        for f in features:\n",
        "            \n",
        "            for s in range(0, signal.shape[1]):\n",
        "                \n",
        "                vec.append(f(signal[:,s][w : w + win_sz]))\n",
        "        \n",
        "        if tp == 'mag':\n",
        "            \n",
        "            vec.append(m)\n",
        "            vec.append(r)\n",
        "            vec.append(subnum)\n",
        "        \n",
        "        mat.append(vec)\n",
        "    \n",
        "    return mat"
      ],
      "metadata": {
        "id": "tX5h-4yHfQRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_DC_component(signal, stim):  ## Remove DC component of signal ##\n",
        "    \n",
        "    for col in range(0, signal.shape[1]):\n",
        "        \n",
        "        s = signal[:, col]\n",
        "        \n",
        "        y = stim == 0\n",
        "        \n",
        "        noise = s[y[:,0]==0]\n",
        "        \n",
        "        dcv = np.mean(noise)\n",
        "        \n",
        "        signal[:, col] -= dcv\n",
        "        \n",
        "    return"
      ],
      "metadata": {
        "id": "o_HUFEEDeagE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_filt(signal, tp):  ## Filter signal using either lp or bp Butterworth filter ##\n",
        "    \n",
        "    if tp == 'lp':\n",
        "    \n",
        "        b, a = btw_lpf(4, 2000, 45)\n",
        "            \n",
        "    elif tp == 'bp':\n",
        "        \n",
        "        b, a = btw_bpf(4, 2000, 20, 450)\n",
        "        \n",
        "    for col in range(0, signal.shape[1]):\n",
        "            \n",
        "        signal[:,col] = scipy.signal.filtfilt(b, a, signal[:, col])\n",
        "    \n",
        "    return\n",
        "    \n",
        "\n",
        "def btw_lpf(order, fs, cutoff):  ## Build Butterworth Bandpass Filter ##\n",
        "    \n",
        "    nyqf = 0.5 * fs\n",
        "    \n",
        "    cut = cutoff / nyqf\n",
        "    \n",
        "    b, a = scipy.signal.butter(order, cut, btype='low')\n",
        "    \n",
        "    return b, a\n",
        "\n",
        "\n",
        "def btw_bpf(order, fs, lowcut, highcut):  ## Build Butterworth Bandpass Filter ##\n",
        "    \n",
        "    nyqf = 0.5 * fs\n",
        "    \n",
        "    low = lowcut / nyqf\n",
        "    \n",
        "    high = highcut / nyqf\n",
        "    \n",
        "    b, a = scipy.signal.butter(order, [low, high], btype='band')\n",
        "    \n",
        "    return b, a"
      ],
      "metadata": {
        "id": "OS3Dsm64e7gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_value(sig):  ## Calculate mean absolute value of signal ##\n",
        "    \n",
        "    return np.mean(np.abs(sig))\n",
        "\n",
        "def mean_absolute_value_slope(mav_vec):  ## Calculate mean absolute value slope ##\n",
        "    \n",
        "    return np.append(mav_vec[1:], mav_vec[0]) - mav_vec\n",
        "\n",
        "def zero_crossings(sig):  ## Calculate zero crossings number of signal ##\n",
        "    \n",
        "    return len(np.where(np.diff(np.signbit(sig)))[0])\n",
        "\n",
        "def slope_sign_changes(sig):  ## Calculate slope sign changes number of signal ##\n",
        "\n",
        "    sgn = np.sign(np.diff(sig))\n",
        "    \n",
        "    ssc = sum((np.roll(sgn, 1) - sgn) != 0)\n",
        "            \n",
        "    return ssc\n",
        "\n",
        "def waveform_length(sig):  ## Calculate waveform length of signal ##\n",
        "    \n",
        "    wl = 0\n",
        "    \n",
        "    wl = np.sum(abs(sig[1:] - sig[:-1]))\n",
        "        \n",
        "    return wl\n",
        "\n",
        "def root_mean_square(sig):  ## Calculate root mean square of signal ##\n",
        "    \n",
        "    return np.sqrt(np.mean(sig)**2)\n",
        "\n",
        "def variance(sig):  ## Calculate variance of signal ##\n",
        "    \n",
        "    return np.mean(sig**2)"
      ],
      "metadata": {
        "id": "aiMtmDApfFop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_features(feat_mat):  ## Standardize feature matrix ##\n",
        "    \n",
        "    for f in range(0, feat_mat.shape[1]-3):\n",
        "        \n",
        "        print(f)\n",
        "        \n",
        "        feat_mat[:,f] = feat_mat[:,f]/np.amax(abs(feat_mat[:,f]))\n",
        "        \n",
        "        mean_val = np.mean(feat_mat[:,f])\n",
        "        std_val = statistics.stdev(feat_mat[:,f])\n",
        "        \n",
        "        feat_mat[:,f] = (feat_mat[:,f] - mean_val) / std_val\n",
        "        \n",
        "        #feat_mat[:,f] = feat_mat[:,f]/np.amax(abs(feat_mat[:,f]))\n",
        "        \n",
        "    print('\\nFeature standardization complete.')\n",
        "                            \n",
        "    return feat_mat"
      ],
      "metadata": {
        "id": "InIKfrJUfXzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM_classifier(feat_mat, tp, fname):  ## Model and train SVM classifier ##\n",
        "\n",
        "    print('SVM' + fname)\n",
        "    \n",
        "    x_train, y_train, x_test, y_test, input_size, output_size = extract_sets_random(feat_mat, tp, False)\n",
        "                \n",
        "    clf = svm.SVC()\n",
        "    \n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(x_test)\n",
        "    \n",
        "    print(\"Testing accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    indiv_acc = cfmat_extraction(y_test, y_pred, fname)\n",
        "    \n",
        "    return clf"
      ],
      "metadata": {
        "id": "XhNHEBT1uetA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN_classifier(feat_mat, tp, lr, fname):  ## Model and train ANN classifier ##\n",
        "\n",
        "    print('ANN ' + fname)\n",
        "    \n",
        "    x_train, y_train, x_test, y_test, input_size, output_size = extract_sets_random(feat_mat, tp, True)\n",
        "    \n",
        "    #x_train, y_train, x_test, y_test, input_size, output_size = filter_amputees(feat_mat, tp, True)\n",
        "    #input_size = len(x_train[0])\n",
        "    \n",
        "    model = tf.keras.Sequential(name='ANN-FF_model')\n",
        "    model.add(tf.keras.layers.Dense(units=512,\n",
        "                                    input_shape=(input_size,),\n",
        "                                    activation='relu',\n",
        "                                    name='input_layer')\n",
        "              )\n",
        "    \n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units=512,\n",
        "                                    activation='relu',\n",
        "                                    name='hidden_layer')\n",
        "              )\n",
        "    \n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(output_size,\n",
        "                                    activation='softmax',\n",
        "                                    name='output_layer')\n",
        "              )\n",
        "    \n",
        "    \n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "    \n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
        "    \n",
        "    history = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        batch_size=2048,\n",
        "                        epochs=1000,\n",
        "                        verbose=1,\n",
        "                        validation_split=0.1,\n",
        "                        shuffle=True,\n",
        "                        validation_freq=1, callbacks=[callback]\n",
        "                        )\n",
        "    \n",
        "    print('Training finalized at epoch', len(history.history['loss']))\n",
        "    \n",
        "    plot_acc_loss(history, fname)\n",
        "    \n",
        "    model.evaluate(x_test, y_test)\n",
        "    \n",
        "    y_pred = model.predict(x_test)\n",
        "   \n",
        "    indiv_acc = cfmat_extraction(y_test, y_pred, fname)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "2qGjtga1vFs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_amputees(feat_mat, tp, ohe): ## take only amputee movements ##\n",
        "    \n",
        "    data = feat_mat\n",
        "    target = feat_mat[:, -3]\n",
        "    \n",
        "    \n",
        "    data, target = shuffle_data_and_target(data, target)\n",
        "    \n",
        "    if ohe == True:\n",
        "        \n",
        "        target_enc, input_size, output_size = one_hot_encoding(data, target)\n",
        "        \n",
        "        x_train, x_test, y_train, y_test = train_test_split(data, target_enc, test_size = 0.1, stratify=target_enc)\n",
        "        \n",
        "        if tp == 'emg':\n",
        "            \n",
        "            x_train = x_train[:,:72]\n",
        "            mask = (x_test[:,-1]==21) | (x_test[:,-1]==22)\n",
        "            x_test = x_test[mask,:72]\n",
        "            y_test = y_test[mask]\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            x_train = x_train[:,:-3]\n",
        "            mask = (x_test[:,-1]==21) | (x_test[:,-1]==22)\n",
        "            x_test = x_test[mask,:-3]\n",
        "            y_test = y_test[mask]\n",
        "        \n",
        "    \n",
        "    else:\n",
        "        \n",
        "        input_size = len(data[0])\n",
        "        output_size = 1\n",
        "        \n",
        "        x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.1, stratify=target)\n",
        "    \n",
        "\n",
        "    return x_train, y_train, x_test, y_test, input_size, output_size  \n",
        "        \n",
        "        \n",
        "def extract_sets_random(feat_mat, tp, ohe): ## Extract train and test sets, apply one hot encoding if necessary ##\n",
        "    \n",
        "    if tp == 'emg':\n",
        "        \n",
        "        data = feat_mat[:, 0:72]\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        data = feat_mat[:, :-3]\n",
        "        \n",
        "    target = feat_mat[:, -3]\n",
        "    \n",
        "    \n",
        "    data, target = shuffle_data_and_target(data, target)\n",
        "    \n",
        "    if ohe == True:\n",
        "        \n",
        "        target_enc, input_size, output_size = one_hot_encoding(data, target)\n",
        "        \n",
        "        x_train, x_test, y_train, y_test = train_test_split(data, target_enc, test_size = 0.1, stratify=target_enc)\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        input_size = len(data[0])\n",
        "        output_size = 1\n",
        "        \n",
        "        x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.1, stratify=target)\n",
        "    \n",
        "\n",
        "    return x_train, y_train, x_test, y_test, input_size, output_size\n",
        "\n",
        "\n",
        "def one_hot_encoding(data, target):  ## Apply one hot encoding to target values ##\n",
        "    \n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(target)\n",
        "    \n",
        "    input_size = len(data[0])\n",
        "    output_size = len(list(le.classes_))\n",
        "    \n",
        "    target_enc = tf.keras.utils.to_categorical(le.transform(target), output_size)\n",
        "    \n",
        "    return target_enc, input_size, output_size\n",
        "    \n",
        "\n",
        "def shuffle_data_and_target(data, target):  ## Shuffle data and target arrays ##\n",
        "    \n",
        "    indices = np.arange(data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    data = data[indices]\n",
        "    target = target[indices]\n",
        "    \n",
        "    return data, target\n",
        "\n",
        "\n",
        "def cfmat_extraction(y_test, y_pred, fname): ## Extract confusion matrix and individual movement accuracy, save both to files #\n",
        "    \n",
        "    img_dir = '*insert image directory location*'\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        \n",
        "        y_pred = np.argmax(y_pred, axis = 1)\n",
        "        y_test = np.argmax(y_test, axis = 1)\n",
        "    \n",
        "    cf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    cf_mat = np.divide(cf_mat,cf_mat.sum(axis=1))\n",
        "    \n",
        "    plt.figure()\n",
        "    \n",
        "    plt.matshow(cf_mat)\n",
        "    \n",
        "    plt.savefig(img_dir + str(fname)+'_cfm1.png', dpi=100)\n",
        "    \n",
        "    plt.figure()\n",
        "    \n",
        "    sn.heatmap(cf_mat, annot=False)\n",
        "    \n",
        "    plt.savefig(img_dir + str(fname)+'_cfm2.png', dpi=100)\n",
        "    \n",
        "    indiv_acc = np.ones(cf_mat.shape[0])\n",
        "    \n",
        "    for i in range(0,len(cf_mat)):\n",
        "        \n",
        "        indiv_acc[i] = cf_mat[i, i] / np.sum(cf_mat[i,:])\n",
        "    \n",
        "    np.savetxt(img_dir + str(fname)+'_ia', indiv_acc)\n",
        "    \n",
        "    return indiv_acc\n",
        "        \n",
        "        \n",
        "def plot_acc_loss(history, fname):  ## Plot accuracy graph ##\n",
        "\n",
        "    img_dir = '*insert image directory location*'\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.savefig(img_dir + str(fname)+'_accplot.png', dpi=100)\n",
        "    \n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.savefig(img_dir + str(fname)+'_lossplot.png', dpi=100)\n",
        "    \n",
        "    \n",
        "    return"
      ],
      "metadata": {
        "id": "rST_xOUnv9mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(activation, init, neurons): ## create model for hyperparameter tuning ##\n",
        "\n",
        "    model = tf.keras.Sequential(name='ANN-FF_model')\n",
        "    model.add(tf.keras.layers.Dense(neurons, input_shape=(720,), activation=activation, name='input_layer')) ## change according to tp\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=activation, name='hidden_layer_1'))\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=activation, name='hidden_layer_2'))\n",
        "    model.add(tf.keras.layers.Dense(40, activation='softmax', name='output_layer'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "def get_best_hyperparams_ANN(feat_mat, tp): ## tune ANN model ##\n",
        "    \n",
        "    x_train, y_train, x_test, y_test, input_size, output_size = extract_sets_random(feat_mat, tp, True)\n",
        "    \n",
        "    model = KerasClassifier(model=create_model, verbose=10)\n",
        "\n",
        "    batch_size = [512, 1024, 2048]\n",
        "    epochs = [250]\n",
        "    optimizer = ['adam']\n",
        "    learning_rate = [0.1, 0.01, 0.001]\n",
        "    activation = ['relu', 'tanh', 'sigmoid']\n",
        "    init = ['uniform','normal','zero']\n",
        "    neurons = [128,256,512]\n",
        "    \n",
        "    param_grid=dict(batch_size=batch_size, \n",
        "                    epochs=epochs, \n",
        "                    optimizer=optimizer,\n",
        "                    optimizer__learning_rate=learning_rate,\n",
        "                    model__activation=activation,\n",
        "                    model__init = init,\n",
        "                    model__neurons = neurons\n",
        "                    )\n",
        "    \n",
        "    grid = GridSearchCV(estimator=model, \n",
        "                        param_grid=param_grid, \n",
        "                        n_jobs=-1, \n",
        "                        cv=3,\n",
        "                        verbose=10)\n",
        "    \n",
        "    grid_result = grid.fit(x_train, \n",
        "                           y_train,\n",
        "                           batch_size=2048,\n",
        "                           verbose=10,\n",
        "                           shuffle=True,\n",
        "                           validation_freq=1\n",
        "                            )\n",
        "\n",
        "    return grid_result.best_params_\n",
        "\n",
        "\n",
        "def get_best_hyperparams_SVM(feat_mat, tp): ## tune SVM model ##\n",
        "    \n",
        "    \n",
        "    x_train, y_train, x_test, y_test, input_size, output_size = extract_sets_random(feat_mat, tp, False)\n",
        "     \n",
        "    kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    C = [0.1, 1, 5, 10, 25, 50, 100]\n",
        "    gamma = [1, 0.1, 0.01, 0.005, 0.001, 0.0005]\n",
        "    \n",
        "    param_grid = dict(kernel=kernel,\n",
        "                      C=C,\n",
        "                      gamma=gamma) \n",
        "           \n",
        "    clf = GridSearchCV(estimator=svm.SVC, param_grid=param_grid)\n",
        "    \n",
        "    clf.fit(x_train, y_train)\n",
        "    \n",
        "    return clf.best_estimator_"
      ],
      "metadata": {
        "id": "Y8ez1k9M27KK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}